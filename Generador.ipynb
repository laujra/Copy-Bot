{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "!pip install --upgrade pip\n",
        "!pip install sentencepiece\n",
        "!pip install gensim\n",
        "!pip install argparse\n",
        "!pip install tokenizers\n",
        "!pip install pathlib\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "Yvp-Jblweadd",
        "outputId": "8d9f9f51-095f-4d98-9f8b-62131555c016"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (22.3.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.97)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting argparse\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.13.2)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6otmqBlGzLh",
        "outputId": "0b51f634-58b5-4938-c92d-3bcb43922568"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50257, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import numpy as np\n",
        "import random\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import os\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tuits=pd.read_excel(\"/content/TuitsNeobancos.xlsx\")"
      ],
      "metadata": {
        "id": "ddIF_54DorcS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuits=(Tuits[\"full_text\"]).tolist()"
      ],
      "metadata": {
        "id": "W4uXZL1Cu7Rk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 280"
      ],
      "metadata": {
        "id": "Ee9-g20xtYb1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SongLyrics(Dataset):  \n",
        "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
        "\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
        "        self.lyrics = []\n",
        "\n",
        "        for row in Tuits[\"full_text\"]:\n",
        "          self.lyrics.append(torch.tensor(\n",
        "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
        "            ))               \n",
        "        if truncate:\n",
        "            self.lyrics = self.lyrics[:20000]\n",
        "        self.lyrics_count = len(self.lyrics)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.lyrics_count\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.lyrics[item]\n",
        "    \n",
        "dataset = SongLyrics(Tuits[\"full_text\"], truncate=True, gpt2_type=\"gpt2\")   "
      ],
      "metadata": {
        "id": "AOnv0XAl5ZYu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
        "        return packed_tensor, False, new_tensor\n",
        "    else:\n",
        "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
        "        return packed_tensor, True, None"
      ],
      "metadata": {
        "id": "4ETEnsgz5wir"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    dataset, model, tokenizer,\n",
        "    batch_size=16, epochs=15, lr=2e-5,\n",
        "    max_seq_len=400, warmup_steps=200,\n",
        "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
        "    test_mode=False,save_model_on_epoch=False,\n",
        "):\n",
        "    acc_steps = 100\n",
        "    device=torch.device(\"cuda\")\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    loss=0\n",
        "    accumulating_batch_count = 0\n",
        "    input_tensor = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(f\"Training epoch {epoch}\")\n",
        "        print(loss)\n",
        "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
        "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
        "\n",
        "            if carry_on and idx != len(train_dataloader) - 1:\n",
        "                continue\n",
        "\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            outputs = model(input_tensor, labels=input_tensor)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "\n",
        "            if (accumulating_batch_count % batch_size) == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "\n",
        "            accumulating_batch_count += 1\n",
        "            input_tensor = None\n",
        "        if save_model_on_epoch:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
        "            )\n",
        "    return model"
      ],
      "metadata": {
        "id": "GGN3n2D15zs2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train(dataset, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr7YKx7652Un",
        "outputId": "0a965db7-778c-4d88-cc88-473ecf04e14c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 0\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 1\n",
            "tensor(3.0246, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 2\n",
            "tensor(3.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:29, 13.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 3\n",
            "tensor(3.5922, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:27, 14.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 4\n",
            "tensor(2.7958, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 5\n",
            "tensor(2.1436, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 6\n",
            "tensor(3.3291, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 7\n",
            "tensor(2.0365, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 8\n",
            "tensor(2.3381, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 9\n",
            "tensor(1.9238, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 10\n",
            "tensor(1.5071, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 11\n",
            "tensor(2.0621, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 12\n",
            "tensor(1.1575, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 13\n",
            "tensor(1.1381, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 14\n",
            "tensor(0.9510, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "393it [00:28, 13.81it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = Tuits.sample(n = 100).reset_index()"
      ],
      "metadata": {
        "id": "jnmyQMCb8TF-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    entry_count=10,\n",
        "    entry_length=30, #maximum number of words\n",
        "    top_p=0.8,\n",
        "    temperature=1.,\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "\n",
        "    filter_value = -float(\"Inf\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "\n",
        "            entry_finished = False\n",
        "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "                outputs = model(generated, labels=generated)\n",
        "                loss, logits = outputs[:2]\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                    ..., :-1\n",
        "                ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "\n",
        "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "                generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
        "                    entry_finished = True\n",
        "\n",
        "                if entry_finished:\n",
        "\n",
        "                    generated_num = generated_num + 1\n",
        "\n",
        "                    output_list = list(generated.squeeze().numpy())\n",
        "                    output_text = tokenizer.decode(output_list)\n",
        "                    generated_list.append(output_text)\n",
        "                    break\n",
        "            \n",
        "            if not entry_finished:\n",
        "              output_list = list(generated.squeeze().numpy())\n",
        "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
        "              generated_list.append(output_text)\n",
        "                \n",
        "    return generated_list\n",
        "\n",
        "#Function to generate multiple sentences. Test data should be a dataframe\n",
        "def text_generation(test_data):\n",
        "  generated_lyrics = []\n",
        "  for i in range(len(test_data)):\n",
        "    x = generate(model.to('cpu'), tokenizer, test_data['full_text'][i], entry_count=1)\n",
        "    generated_lyrics.append(x)\n",
        "  return generated_lyrics\n",
        "\n",
        "#Run the functions to generate the lyrics\n",
        "generated_lyrics = text_generation(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdrkku2O6zSu",
        "outputId": "16abc731-d995-4df7-e257-bc3ead0ced58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.68s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.69s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.28it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.28it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.26it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.31it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.19it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.77it/s]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.87s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.23s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.58s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.46it/s]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.98s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.32s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.72s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.63s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.08s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.93s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.70s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.60s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.52it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 11.51it/s]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.35s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.33it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.99it/s]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.61s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.44s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.30it/s]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.85s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.48s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.63s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.73it/s]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.82s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.91it/s]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.78s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.65it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.93s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.24s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.40s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  7.48it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.27it/s]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.88s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.58s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.64s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.12it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.49it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.34it/s]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.65s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.05s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.12it/s]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.38s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.03s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  3.07it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.64it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.53it/s]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.36s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.75s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.02s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.26s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.61it/s]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.42s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.80s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.71s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.98s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.59s/it]\n",
            "100%|██████████| 1/1 [00:02<00:00,  2.55s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.88s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.76s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.19s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.70s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.96s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.75it/s]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.60s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.78s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.36it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  7.02it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  2.95it/s]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.12s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.81s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.42it/s]\n",
            "100%|██████████| 1/1 [00:07<00:00,  8.00s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.36s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.63it/s]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.12s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.98s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.63s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.94s/it]\n",
            "100%|██████████| 1/1 [00:04<00:00,  4.82s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00,  5.82it/s]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.08s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_generations=[]\n",
        "\n",
        "for i in range(len(generated_lyrics)):\n",
        "  a = test_set['full_text'][i].split()[-30:] #Get the matching string we want (30 words)\n",
        "  b = ' '.join(a)\n",
        "  c = ' '.join(generated_lyrics[i]) #Get all that comes after the matching string\n",
        "  my_generations.append(c.split(b)[-1])\n",
        "\n",
        "test_set['Generated_text'] = my_generations\n",
        "\n",
        "\n",
        "#Finish the sentences when there is a point, remove after that\n",
        "final=[]\n",
        "\n",
        "for i in range(len(test_set)):\n",
        "  to_remove = test_set['Generated_text'][i].split('.')[-1]\n",
        "  final.append(test_set['Generated_text'][i].replace(to_remove,''))\n",
        "\n",
        "test_set['Generated_text'] = final"
      ],
      "metadata": {
        "id": "MQNltZ-cCG4R"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator_en_es = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvtrKdVeC-Yi",
        "outputId": "8b994907-7290-4930-d033-eb26837246f0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator_en_es(my_generations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU2-6uohHb0F",
        "outputId": "3411fd72-fce4-4c47-d5ac-591a6c479326"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'translation_text': 'estamos tan apenados por las molestias que hemos causado su dispositivo. estamos apenados si usted experimentó cualquier problema en su orden o la aplicación.'},\n",
              " {'translation_text': 'Queremos darle la paz de la mente que hoy seguimos asistiendo. usted puede encontrarnos en el chat de la aplicación de 10 a.m. a 4 p.m., a través de nuestras redes sociales oficiales o por correo electrónico a hola@neobank.com.ar .endoftext'},\n",
              " {'translation_text': 'Sí, por supuesto sabemos que la inflación puede fluctuar dependiendo del precio de una mercancía. Una moneda fuerte tiene una base más pequeña de reservas y por lo tanto'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'Cómo construir crédito en la universidad; una etiqueta de hilo al estudiante de la universidad que necesita verlo.'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'Twitter.com/9f9kj57ppa'},\n",
              " {'translation_text': '¿Qué juegos quieres que jueguen tus hijos? ¿Puedes mantener las luces encendidas? ¿Cuándo puedes empezar a recibir tus regalos? hemos reunido tu endoftexto'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'pic.twitter.com/lx9bbkap1dv'},\n",
              " {'translation_text': '...  fin del texto '},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': '¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡¡!!!!!!'},\n",
              " {'translation_text': 'Únete a la discusión de abajo, y si tienes otras ideas o peticiones de características, ¡solo deja un comentario más abajo!'},\n",
              " {'translation_text': 'twitter.com/XXrclmm4p.1 Ran: Fue impresionante. Todos los fans que hicieron que esto sucediera.'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': '.@firearena https://t.co/qW5e1xbs3qv. .@beneliasendoftext'},\n",
              " {'translation_text': 'Si es uno de los que dice \"eso es para lo que trabajo\", ha llegado el momento de comprar con nuestro descuento exclusivo para #neobank clientes https://t.co/x4r5ivu3mo. https://t.co/h9g5tk6kmje.endoftext'},\n",
              " {'translation_text': '— Deepak Chopra (@deepakchopra) 28 de noviembre de 2016'},\n",
              " {'translation_text': 'Cuando el orgullo del neobanco paga literalmente'},\n",
              " {'translation_text': '2xu7zendoftext'},\n",
              " {'translation_text': 'https://t.co/mjd9r6sa1j3.endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'Sus servicios son un pase.'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'Rt @dallasmavs: bienvenido al retroplex. @neobank  #mffl https://t.co/xpebiuxqtu.endoftext'},\n",
              " {'translation_text': '¡Si tienes alguna pregunta, envíamelas en Twitter @dustin_fecalice y trataré de responderlas!'},\n",
              " {'translation_text': 'Aidyne (@Aidyne) 30 de julio de 2017  todos los que retuitearon @UncleRyan en el endotexto'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': '(ualá bis i nái ná i uilá ahdz oí) Larga en la distancia, giro y endoftext'},\n",
              " {'translation_text': 'pic.twitter.com/tbpT8Wk2p2t.'},\n",
              " {'translation_text': 'Matthew Asenath (@msattasenath) May 5, 2017 AGUSTA, Ga., (AP)'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'Youtube.com/totemanrt #deckscreependoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'No tengo un mod que realmente se preocupa cuánto tiempo espero para nuevas texturas para estar listo antes de que voy a abrirlos en mi nuevo mundo [09endoftext'},\n",
              " {'translation_text': 'alltidep.com/h/t...'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'en 1 minuto 20 segundos #eurekalert (@0craig) 30 de agosto de 2016 Si quieres saber más'},\n",
              " {'translation_text': '#farage3 #wildcardes #sean https://t.co/u04zw7g1zk0. endoftext'},\n",
              " {'translation_text': 'La gente como Gaspard, el vicepresidente de finanzas del banco y el novio a largo plazo del ganador, son generalmente un puñado de clics lejos'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': '\"¿B.A. le pregunta qué está haciendo su padre y quién está trabajando para ella o toda la familia?\", pregunta Shawne en hisendoftext'},\n",
              " {'translation_text': '#iffy https://t.co/I4gSG1endoftext'},\n",
              " {'translation_text': 'A pesar de haber sido elegida en el papel de Miguel, la buena gente atendoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'Únase a nosotros para ver esta hermosa historia... en #neobank sabemos que a veces un error puede costarnos mucho, es por eso que hoy celebramos la honestidad de la señora al devolver el dinero a Santiago. #bienvenida al nuevo mundo. https://t.co/sqnj9ohp3x.'},\n",
              " {'translation_text': 'lalá  - gracias por su apoyo!  la plataforma se construyó con tal dedicación yendoftext'},\n",
              " {'translation_text': 'Sígueme. #rocksheep #ralcifer #rufhumanrights https://t.co/w0uew6klfendoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'tomó @trademark_nation para mí para ver que él tenía este tig, e hizo un photendoftext'},\n",
              " {'translation_text': '#theweekhenome https://t.co/aldwq6rf4jw — DARD #DUNGEONWATERendoftext'},\n",
              " {'translation_text': '¡Ahora puede pedir sus hogares de neobank!   en sólo 3 pasos entrando de los servicios  no le pedimos la información de la tarjeta  usted pide y paga directamente de su disponible  la expectativa de su almuerzo se cumple con su pedido https://t.co/ahexzrfpia https://t.co/9s4tqkxfmo.endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'Solía conducir allí todos los días. Echo de menos el resto de la ciudad y los alrededores.'},\n",
              " {'translation_text': '1b7d1b3'},\n",
              " {'translation_text': 'Entiendo que es extremadamente caro comprar una imagen viral. ¿Por qué etiquetarías en una imagen si ya puedes vincular la imagen a endoftext'},\n",
              " {'translation_text': '1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'tal vez tus amigos en este club están interesados... o son ellos. pero no confiarán en ti y tendré que decir adiós.'},\n",
              " {'translation_text': 'Este botón abre un diálogo que muestra imágenes adicionales para este producto con la opción de acercar o alejar.'},\n",
              " {'translation_text': '#Huge1 https://t.co/njduuy9tjx4'},\n",
              " {'translation_text': 'esta temporada el partido #neobank fue impecable . son 2do clasificado y los mejores equipos de la región, esto es el endoftext'},\n",
              " {'translation_text': '- No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.'},\n",
              " {'translation_text': 'podemos darle \"dólar de EE.UU.\" envío gratuito! +...másendoftext'},\n",
              " {'translation_text': 'LICHE.COM: Las tarjetas C6 son realmente los mejores cambiadores de dinero, porque trabajan como tarjetas de crédito.'},\n",
              " {'translation_text': 'https://t.co/g6ph5z9tqk. https://t.co/pw3xtendoftext'},\n",
              " {'translation_text': '2 — ¿Judíos de alta tecnología?'},\n",
              " {'translation_text': '#poshup  — gannitrios_plasti (@georgiplasti) 6 de agosto de 2017 Theendoftext'},\n",
              " {'translation_text': '(Algunos usuarios dicen que ô-show-a está en la versión más antigua, que se puede descargar de la repo. Recomendamos utilizar el último estableendoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': '¿Cuáles son tus metas y objetivos en el #SnappinLeague?'},\n",
              " {'translation_text': 'Querido sergio, sabemos que con el frío en bogotá debemos recurrir a medidas extremas... por esta razón, te invitamos a mejorar el plan con una dirección y la mejor, pagando con tu #neobank  https://t.co/ju8ztdqdyq.'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'más 1 de 5 por Aljamij de RHarmada de Oklahoma City, OK, USA @rahmada4 rua yourendoftext'},\n",
              " {'translation_text': '#menforthepanda https://t.co/endoftext'},\n",
              " {'translation_text': '1/ estamos un paso más cerca de los pagos instantáneos que se convierten en la norma en la eu. esta es una gran noticia para los consumidores. un hilo corto (celebratorio). https://t.co/9b4iu1zdpm.endoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': 'No es que no veamos estas cosas, nunca hacemos eso. Estamos tan ocupados con la calidad de vida que donendoftext'},\n",
              " {'translation_text': 'Leviathan (@hobbypiritu9) Julio 25, 2017 El 30% de todas las transferencias neobancarias son totalmente instantáneas'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': '¿Los militares le pusieron una placa al final del texto?'},\n",
              " {'translation_text': 'Adam Fenix (@fenix69) July 12, 2017 El fútbol estadounidense está a punto de tener su primera mujer'},\n",
              " {'translation_text': '@Cogswain'},\n",
              " {'translation_text': 'Esta es la cibernética más aterradora que hayamos visto antes en cualquier continente.'},\n",
              " {'translation_text': 'Genial. https://t.co/p7y6zvsduhy pic.twitter.com/9dpafnyob4yendoftext'},\n",
              " {'translation_text': 'endoftext'},\n",
              " {'translation_text': '5 miirad bc pueden arrancar el corazón. pic.twitter.com/DjknQdU3l48 — Cheendoftext'}]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}